### Module 8 - Designing with appropriate state preservation and management options
 - Ways mule application can maintian state - Mule event, object store, vm queues, batch job scope queues, file based persistence and external data store
 - Object store is typically accessed via a object store connector. But an externalized object store implementation might provide other communication mechanisms, such as a secure REST connection. Configured as persistent or non-persistent
 - Mule applications deployed to customer-hosted or runtime fabric could use the OSv2 REST API to access a cloudhub-deployed application's persistent object stores
 - Private object store can be configured by a particular component in a mule application to hide its object store data from any other component in the mule application
 - JMS, AMQP, Anypoint MQ or DB - More durable and more reliable state management option. Communication between multiple mule applications or with non-mule applications
 -  File based persistence cloudhub - /tmp, runtime fabric - /tmp and customer hosted - /tmp and /otherDirs
 -  External store - database, sftp server, redis, s3 bucket and external in-memory grid
 -  Comparisons
 -  OSV2 - unlimited keys, 10mb max value size. Preserved upon application redeployment and restarts. Deleted upon application deletion. 30 days and 10 TPS per app for free tier
 -  Currently OSv2 uses dyanamo DB and vm queue uses amazon sqs
 -  Customer hosted state management - Performance profile and reliable profile
 -  Batch queue bypass the cluster-replication and do not use hazel cast
 -  Cache scope - Does not cache consumable payload such as non repeatable stream
 -  Watermark
 -  Idempotent message validator

### Module 9 - Designing effective logging and monitoring
> API Analytics and API Monitoring
 - Access management contains the audit log. Organization owner can access all audit logs. Can be used to track access violations. Retained in anypoint platform for 6 years
 - Log levels - DEBUG(async), TRACE(async), INFO(async), WARN(sync) and ERROR(sync)
 - Cloudhub truncates log messages - Upto 100 MB per mule application and per worker - At most 30 days
 - Difference between cloudhub and customer hosted runtime planes - System.out messages will be available in application logs for cloudhub but available in system logs for customer hosted runtim plane
 - Runtime manager can override the log level for a deployed mule application without redployment :triangular_flag_on_post:	
 - Tracing log messages using correlation ID (X-Correlation-ID)
 - By default cloudhub replaces a mule application's log4j2.xml file with the a cloudhub provided log4j2.xml file. Send logs(only application logs and not system logs) to external logging systems using a custom log appender by raising a request to mulesoft support to get the option of disabling cloudhub logs
 - As another option, create a custom aggregator application which fetch logs using cloudhub APIs and send to an external system - Generally not recommended
 - Anypoint monitoring. Platinum - Application performance monitoring and custom metrics & events. Titanium - Application performance monitoring, log management, custom metrics & events, dedicated monitoring infrastructure and enhanced support.
 - Cloudhub - enable/disable via anypoint monitoring and runtime manager. On-premises - Anypoint monitoring agent. Runtime fabric - By default all applications are enabled for anypoint monitoring.
 - Default built in dashboard - Inbound & outbound events, Performance, Infrastructure, JVM and Failures
 - Alerts - Basic(Different metrics are available sources(Mule applications vs servers)) and Advanced(Can be created on widgets from custom dashbaords). Can send notification to email addresses. These alerts are distinct from API manager and runtime manager alerts
 - Custom metrics connector can generate metrics from a flow
 - Monitoring applications from public or private locations
 - Anypoint analytics - Request by date, location, application and platform. Custom dashboards - create charts which displays request, request size, response size and response time
 - Alerts can be configured in runtime manager. Custom notifications can be generated by a mule application using the cloudhub connector, which can generate custom alerts in runtime manager
 - Runtime manager doesn't support alerting on RTF deployed applications

### Module 12 - Designing for reliability goals
 - Reliability aspires to have zero message/data loss after a mule application stops or crashes. A reliability pattern can be implemented to achieve reliability goals using until successful scope, reconnection strategies, redelivery policy, MULE:REDELIVERY_EXHAUSTED error type, transactions, error handling and first successful router
 - Unitl successful scope repeatedly triggers the code within the scope until succeeds or until a maximum number of retries are exceeded. May fail for temporary reasons and may succeed upon retry. Re-executing permanent failures unneccessarily pollutes logs and delay returing failures
 - System errors are thrown when a connection to an external system fails. To retry connection failures, mule connectors can set a connection strategy
 - A redelivery policy can be configured on the event source to specify the number of the time the same event can be processed by the flow before raising a REDELIVERY_EXHAUSTED error. Can't be configured on the scheduler
 - Reliability pattern for non-transactional system. Splits the processing between acquistion flow and processing flow using the persistent queues

### Module 13 - Designing for high-availability goals
 - High availability - How to keep the overall system operational when a system component failes. Usually acheived with multiple levels of fault tolerances and/or load balancing
 - Disaster recovery - How to restore a system to a previous acceptable state after a natural or man-made disaster
 - High availability can be acheived by horizontally scaling to multiple mule runtimes. HA goals for customer-hosted or runtime fabric can be met via load balancing and/or clustering. HA can be acheived using cloudhub multiple workers. Workers will be spread across different availability zones in the same AWS region. Mule application data can be shared between workers using the object store v2 service and vm queues with persistent queues enabled in the runtime manager
 - HTTPS request are load balanced automatically through shared or dedicated load balancers
 - In cloudhub, each persistent vm queue is listened by multiple cloudhub workers. But each message is read and processed atleast once by only one cloudhub worker and duplicate processing is possible. Persistent queues do not guarantee one-time-only message delivery. Duplicate messages may be sent
 - Runtime manager can define server groups or cluster. Server groups is just an administrative grouping of isolated mule runtimes. A cluster provides additional guarantees to prevent contention between the mule runtime
 - Unclustered load balancing for HA and performance in customer-hosted runtime planes
 - Cluster is a set of customer hosted mule runtimes that acts as a unit. Servers in a cluster communicate through a distributed shared memory grid. All clusters nodes work in active-active mode and there is a primary node where schedulers, jms listeners etc run
 - A cluster can be tuned to be either performant or reliable. Clusters are either unicast or multicast. Cluster uses hazelcast to create a distributed shared memory grid
 - Ngnix and apache web server - third party load balancers for customer-hosted mule runtime
 - Runtime fabric has docker/k8s managed load balancer
 - Load balancing for https, vm(cluster and standalone), jms
 - Kubernetes cluster

### Module 14 - Optimizing the performance of deployed mule applications
 - Performance goals are atleast partially dictated by service level aggrements. It is important to not to over-optimize and aim to vastly exceed the performance goals
 - Use application performance monitoring tools, including the anypoint platform dashboards, visual vm/jconsole or similar. Commerial tools like app dynamics, new relic etc
 - Application profiling in mule applications is often performed for memory issues and application unresponsiveness
 - Scaling options, network communication and protocols
 - A mule application can be scaled out to a maximum of eight workers and/or scaled up to sixteen vcores
 - Configuring autoscaling in cloudhub(ELA customers only)
 - Performance features of common mule applications (Batch, scheduler and event/messaging queues)
 - Logging default to async for performance
 - HTTPS, JMS, Database, VM connectors

### Questions
